{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\frase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "<ipython-input-6-903feb3b24aa>:99: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarities.append(new_prompt.similarity(new_sentence))\n",
      "C:\\Users\\frase\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values:  [0 1 1 ... 1 1 1]\n",
      "Accuracy score:  0.8179310344827586\n",
      "Precision score:  0.8307013885413365\n",
      "Recall score:  0.8179310344827586\n",
      "Baseline Accuracy score:  0.7793103448275862\n",
      "Baseline Precision score:  0.6073246135552913\n",
      "Baseline Recall score:  0.7793103448275862\n",
      "Confusion Matrix:\n",
      "[[  64  256]\n",
      " [   8 1122]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frase\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import argparse\n",
    "import nltk\n",
    "import pickle\n",
    "import scipy\n",
    "import spacy\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tags = [',','.',':','``',\"''\",'CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']\n",
    "   \n",
    "def position_features(data):\n",
    "    dataframe = data\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        paragraph = row['Source Paragraph']\n",
    "        sentence = row['Sentence']\n",
    "        start_pos = paragraph.find(sentence)\n",
    "        end_pos=sentence.find(sentence[-1:]) + start_pos\n",
    "        \n",
    "        start_positions.append(start_pos)\n",
    "        end_positions.append(end_pos)\n",
    "        \n",
    "    dataframe['Relative Sentence Start Pos'] = start_positions\n",
    "    dataframe['Relative Sentence End Pos'] = end_positions\n",
    "        \n",
    "               \n",
    "def token_features(data):\n",
    "    dataframe = data\n",
    "    \n",
    "    part_of_speech_tokens = []\n",
    "    most_common_pos_token = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentence = row['Sentence']\n",
    "        \n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        pos_tokens = nltk.pos_tag(sentence_tokens)\n",
    "        \n",
    "        tokens, pos_tags = zip(*pos_tokens)\n",
    "        \n",
    "        part_of_speech_tokens.append(pos_tags)\n",
    "        most_common_pos_token.append(max(set(pos_tags), key=pos_tags.count))\n",
    "        \n",
    "    dataframe['Sentence POS Tokens'] = part_of_speech_tokens\n",
    "    dataframe['Most Common POS Token'] = most_common_pos_token\n",
    "\n",
    "\n",
    "def similarity_features(data):\n",
    "    dataframe = data\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        essay_id = row['Essay ID']\n",
    "        prompt_dataframe = dataframe.loc[(dataframe['Essay ID'] == essay_id)& (dataframe['Paragraph Number'] == 1)]\n",
    "        prompt = prompt_dataframe.iloc[0]['Sentence']\n",
    "        sentence = row['Sentence']\n",
    "        prompt_doc = nlp(prompt.lower())\n",
    "        sentence_doc = nlp(sentence.lower())\n",
    "        \n",
    "        prompt_result = []\n",
    "        sentence_result = []\n",
    "        \n",
    "        #Following code was obtained from a tutorial - https://medium.com/better-programming/the-beginners-guide-to-similarity-matching-using-spacy-782fc2922f7c\n",
    "        for token in prompt_doc:\n",
    "            if token.text in nlp.Defaults.stop_words: \n",
    "                continue\n",
    "            if token.is_punct:\n",
    "                continue\n",
    "            prompt_result.append(token.text)\n",
    "            \n",
    "        for token in sentence_doc:\n",
    "            if token.text in nlp.Defaults.stop_words: \n",
    "                continue\n",
    "            if token.is_punct:\n",
    "                continue\n",
    "            sentence_result.append(token.text)\n",
    "        \n",
    "        new_prompt = nlp(\" \".join(prompt_result))\n",
    "        new_sentence = nlp(\" \".join(sentence_result))\n",
    "        \n",
    "       \n",
    "        similarities.append(new_prompt.similarity(new_sentence))\n",
    "    dataframe['Sentence Similarity To Prompt'] = similarities\n",
    "\n",
    "train = pd.read_pickle(\"./train.pkl\")\n",
    "test = pd.read_pickle(\"./test.pkl\")\n",
    "\n",
    "test_essay_id = 4\n",
    "test_essay = test.loc[(test['Essay ID'] == test_essay_id)]\n",
    "\n",
    "position_features(train)\n",
    "token_features(train)\n",
    "similarity_features(train)\n",
    "\n",
    "position_features(test)\n",
    "token_features(test)\n",
    "similarity_features(test)\n",
    "\n",
    "\n",
    "feature_columns=['Paragraph Number', 'Sentence', 'Sentence Similarity To Prompt', 'Most Common POS Token']\n",
    "\n",
    "tf = TfidfVectorizer(max_features = 800,strip_accents = 'ascii',stop_words = 'english',)\n",
    "le = preprocessing.LabelEncoder()\n",
    "pos_encoder = preprocessing.LabelEncoder()\n",
    "pos_encoder.fit(pos_tags)\n",
    "\n",
    "x = train.loc[:, feature_columns]\n",
    "y = train.loc[:, ['Argumentative Label']]\n",
    "x_sentences = x['Sentence']\n",
    "\n",
    "x_sentences_vectorized = tf.fit_transform(x_sentences)\n",
    "x_vectorized_dataframe = pd.DataFrame(x_sentences_vectorized.todense(), columns=tf.get_feature_names())\n",
    "x_concat = pd.concat([x, x_vectorized_dataframe], axis=1)\n",
    "x_final = x_concat.drop(['Sentence'], axis=1)\n",
    "\n",
    "x_pos_encoded = pos_encoder.transform(x['Most Common POS Token'])\n",
    "x_final['Most Common POS Token'] = x_pos_encoded\n",
    "\n",
    "y_binarized = le.fit_transform(y)\n",
    "y['Argumentative Label'] = y_binarized\n",
    "\n",
    "x_new = test.loc[:, feature_columns]\n",
    "y_new = test.loc[:, ['Argumentative Label']]\n",
    "x_new_sentences = x_new['Sentence']\n",
    "\n",
    "x_new_sentences_vectorized = tf.transform(x_new_sentences)\n",
    "x_new_vectorized_dataframe = pd.DataFrame(x_new_sentences_vectorized.todense(), columns=tf.get_feature_names())\n",
    "x_new_concat = pd.concat([x_new, x_new_vectorized_dataframe], axis=1)\n",
    "x_new_final = x_new_concat.drop(['Sentence'], axis=1)\n",
    "\n",
    "x_new_pos_encoded = pos_encoder.transform(x_new['Most Common POS Token'])\n",
    "x_new_final['Most Common POS Token'] = x_new_pos_encoded\n",
    "\n",
    "y_new_binarized = le.transform(y_new)\n",
    "y_new['Argumentative Label'] = y_new_binarized\n",
    "\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(x_final,y.values.ravel())\n",
    "\n",
    "pickle.dump(tf, open(\"tfidf.pickle\", \"wb\"))\n",
    "pickle.dump(pos_encoder, open(\"pos_encoder.pickle\", \"wb\"))\n",
    "pickle.dump(le, open(\"arg_label_encoder.pickle\", \"wb\"))\n",
    "pickle.dump(naive_bayes, open(\"component_identification_model.pickle\", \"wb\"))\n",
    "\n",
    "predictions = naive_bayes.predict(x_new_final)\n",
    "\n",
    "test['Predicted Argumentative Label'] = predictions\n",
    "test.to_pickle(\"essay_components_identified.pkl\")\n",
    "\n",
    "baseline = predictions\n",
    "baseline = np.where(baseline < 1, 1, baseline)\n",
    "\n",
    "c_m = confusion_matrix(y_new.values.ravel(), predictions)\n",
    "\n",
    "print('Predicted Values: ', predictions)\n",
    "print('Accuracy score: ', accuracy_score(y_new.values.ravel(), predictions))\n",
    "print('Precision score: ', precision_score(y_new.values.ravel(), predictions, average='weighted'))\n",
    "print('Recall score: ', recall_score(y_new.values.ravel(), predictions, average='weighted'))\n",
    "print('Baseline Accuracy score: ', accuracy_score(y_new.values.ravel(), baseline))\n",
    "print('Baseline Precision score: ', precision_score(y_new.values.ravel(), baseline, average='weighted'))\n",
    "print('Baseline Recall score: ', recall_score(y_new.values.ravel(), baseline, average='weighted'))\n",
    "print('Confusion Matrix:')\n",
    "print(c_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
