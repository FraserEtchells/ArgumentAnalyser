{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\frase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import argparse\n",
    "import nltk\n",
    "import pickle\n",
    "import scipy\n",
    "import spacy\n",
    "import string\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "list_of_pos_tags = [',','.',':','``',\"''\",'CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']\n",
    "\n",
    "#Run this feature first so we have each sentences lemmatized and their N-grams labelled\n",
    "def tokenisation_features(data):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    lemmatized_sentences_tokens = []\n",
    "    lemmatized_sentences_joined = []\n",
    "    bigrams = []\n",
    "    for index, row in data.iterrows():\n",
    "        sentence = row['Sentence']\n",
    "        tokenized = nlp(sentence)\n",
    "        sentence_lemmatized = []\n",
    "        for word in tokenized:\n",
    "            sentence_lemmatized.append(word.lemma_)\n",
    "        \n",
    "        lemmatized_sentences_tokens.append(sentence_lemmatized)\n",
    "        lemmatized_sentences_joined.append(' '.join(sentence_lemmatized))\n",
    "        sentence_bigrams = list(nltk.bigrams(sentence_lemmatized))\n",
    "        bigrams.append(sentence_bigrams)\n",
    "       \n",
    "    data['Lemmatized Sentence'] = lemmatized_sentences_joined\n",
    "    data['Lemmatized Sentence Tokens'] = lemmatized_sentences_tokens\n",
    "    data['Lemmatized Sentence Bigrams'] = bigrams\n",
    "\n",
    "def part_of_speech_features(data):\n",
    "    \n",
    "    for tag in list_of_pos_tags:\n",
    "        string_tag = \"Distribution of \" + tag + \" POS Tag\"\n",
    "        data[string_tag] = 0.0\n",
    "        \n",
    "    part_of_speech_tokens = []\n",
    "    most_common_pos_token = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        dict_of_occurences = {}\n",
    "        sentence_tokens = row['Lemmatized Sentence Tokens']\n",
    "        \n",
    "        pos_tokens = nltk.pos_tag(sentence_tokens)\n",
    "        \n",
    "        tokens, pos_tags = zip(*pos_tokens)\n",
    "        \n",
    "        part_of_speech_tokens.append(pos_tags)\n",
    "        \n",
    "        for curr_tag in pos_tags:\n",
    "            curr_tag_occurences = pos_tags.count(curr_tag)\n",
    "            #simple bug fix - these symbols keep occuring randomly\n",
    "            if curr_tag == \"(\":\n",
    "                continue\n",
    "            if curr_tag == \")\":\n",
    "                continue\n",
    "            if curr_tag == \"$\":\n",
    "                continue\n",
    "            if curr_tag in dict_of_occurences:\n",
    "                continue\n",
    "            else:\n",
    "                dict_of_occurences[curr_tag] = curr_tag_occurences\n",
    "                string_curr_tag = \"Distribution of \" + curr_tag + \" POS Tag\"\n",
    "                data.loc[index, string_curr_tag] = curr_tag_occurences / len(pos_tags)\n",
    "            \n",
    "def positional_features(data):\n",
    "    \n",
    "    within_introduction = []\n",
    "    within_conclusion = []\n",
    "    essays_in_dataframe = set()\n",
    "    \n",
    "    #When iterating through all of the data, also append the essay id to a list, with only unique values, so we know the essays being used in this block.\n",
    "    for index, row in data.iterrows():\n",
    "        paragraph = row['Paragraph Number']\n",
    "        essays_in_dataframe.add(row['Essay ID'])\n",
    "        \n",
    "        if paragraph == 2:\n",
    "            within_introduction.append(1)\n",
    "            within_conclusion.append(0)\n",
    "            \n",
    "        elif paragraph == row['Total Paragraphs']:\n",
    "            within_introduction.append(0)\n",
    "            within_conclusion.append(1)\n",
    "            \n",
    "        else:\n",
    "            within_introduction.append(0)\n",
    "            within_conclusion.append(0)\n",
    "    data['Sentence Within Introduction'] = within_introduction\n",
    "    data['Sentence Within Conclusion'] = within_conclusion\n",
    "    \n",
    "    number_of_components_proceeding = []\n",
    "    number_of_components_preceeding = []\n",
    "    \n",
    "    #Go through each essay and each paragraph. Total number of components in the paragraph = all sentences in the paragraph\n",
    "    #Want to extract the order of components\n",
    "    for id,curr_essay_id in enumerate(essays_in_dataframe): \n",
    "        curr_essay = data.loc[(data['Essay ID'] == curr_essay_id)]\n",
    "        curr_essay_total_paragraphs = curr_essay['Total Paragraphs'].values[0]\n",
    "        for curr_paragraph_number in range(curr_essay_total_paragraphs):\n",
    "            curr_paragraph = curr_essay.loc[(curr_essay['Paragraph Number'] == curr_paragraph_number + 1)]\n",
    "            total_components_in_paragraph = len(curr_paragraph.index)\n",
    "            for curr_sentence_number in range(total_components_in_paragraph):\n",
    "                number_of_components_proceeding.append(total_components_in_paragraph - (curr_sentence_number+1))\n",
    "                number_of_components_preceeding.append(curr_sentence_number)\n",
    "                \n",
    "    data['Number of Proceeding Components'] = number_of_components_proceeding\n",
    "    data['Number of Preceding Components'] = number_of_components_preceeding\n",
    "            \n",
    "def first_person_indicators_features(data):\n",
    "    \n",
    "    first_person_indicators = set(['i', 'myself', 'my', 'mine'])\n",
    "    presence_of_indicators = []\n",
    "    count_of_indicators = []\n",
    "    #Load in each sentence. Go through each word. If the current word is within the list, set presence to true and count + 1\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        sentence = row['Sentence']\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        current_count = 0\n",
    "        indicator_present = False\n",
    "        \n",
    "        for word in sentence_tokens:\n",
    "            lowercase_word = word.lower()\n",
    "            if lowercase_word in first_person_indicators:\n",
    "                indicator_present = True\n",
    "                current_count += 1\n",
    "                \n",
    "        if indicator_present == True:\n",
    "            presence_of_indicators.append(1)\n",
    "        else:\n",
    "            presence_of_indicators.append(0)\n",
    "            \n",
    "        count_of_indicators.append(current_count)\n",
    "    data['First Person Indicator Present'] = presence_of_indicators\n",
    "    data['First Person Indicator Count'] = count_of_indicators\n",
    "    \n",
    "\n",
    "def forward_indicator_feature(data):\n",
    "    #therefore/thus/consequently indicate the component after the indicator may be a claim\n",
    "    #take each word, make a copy lowercase, check if it is in list\n",
    "    forward_indicators = ['therefore' , 'thus', 'consequently']\n",
    "    presence_of_indicators = []\n",
    "    for index, row in data.iterrows():\n",
    "        sentence = row['Sentence']\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        current_count = 0\n",
    "        indicator_present = False\n",
    "        \n",
    "        for word in sentence_tokens:\n",
    "            lowercase_word = word.lower()\n",
    "            if lowercase_word in forward_indicators:\n",
    "                indicator_present = True\n",
    "                \n",
    "        if indicator_present == True:\n",
    "            presence_of_indicators.append(1)\n",
    "        else:\n",
    "            presence_of_indicators.append(0)\n",
    "        \n",
    "    data['Forward Indicator Present'] = presence_of_indicators\n",
    "    \n",
    "def backward_indicator_feature(data):\n",
    "    #in addition/because/additionally indicate the component after the indicator may be a premise\n",
    "   \n",
    "    backward_indicators = ['in addition', 'because', 'additionally']\n",
    "    presence_of_indicators = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        lower_case_sentence = row['Sentence'].lower()\n",
    "        indicator_present = False\n",
    "        \n",
    "        for i in range(len(backward_indicators)):\n",
    "            if (lower_case_sentence.find(backward_indicators[i]) != -1):\n",
    "                indicator_present = True\n",
    "        \n",
    "        if indicator_present == True:\n",
    "            presence_of_indicators.append(1)\n",
    "        else:\n",
    "            presence_of_indicators.append(0)\n",
    "            \n",
    "    data['Backward Indicator Present'] = presence_of_indicators\n",
    "        \n",
    "    \n",
    "def thesis_indicator_feature(data):\n",
    "    #in my opinion/I believe indicate a component after the indicator may be a major claim\n",
    "\n",
    "    thesis_indicators = ['in my opinion','in my honest opinion' ,'i believe', 'i firmly believe', 'i strongly believe']\n",
    "    \n",
    "    presence_of_indicators = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        lower_case_sentence = row['Sentence'].lower()\n",
    "        indicator_present = False\n",
    "        \n",
    "        for i in range(len(thesis_indicators)):\n",
    "            if (lower_case_sentence.find(thesis_indicators[i]) != -1):\n",
    "                indicator_present = True\n",
    "        \n",
    "        if indicator_present == True:\n",
    "            presence_of_indicators.append(1)\n",
    "        else:\n",
    "            presence_of_indicators.append(0)\n",
    "            \n",
    "    data['Thesis Indicator Present'] = presence_of_indicators\n",
    "        \n",
    "                \n",
    "train = pd.read_pickle(\"./train.pkl\")\n",
    "test = pd.read_pickle(\"./test.pkl\")\n",
    "\n",
    "test_essay_id = 4\n",
    "test_essay = test.loc[(test['Essay ID'] == test_essay_id)]\n",
    "\n",
    "feature_columns=['Lemmatized Sentence','Paragraph Number', 'Sentence Within Introduction', 'Sentence Within Conclusion', 'Number of Proceeding Components', 'Number of Preceding Components' , 'First Person Indicator Present', 'First Person Indicator Count', 'Forward Indicator Present', 'Backward Indicator Present', 'Thesis Indicator Present']\n",
    "for curr_tag in list_of_pos_tags:\n",
    "    feature_columns.append(\"Distribution of \" + curr_tag + \" POS Tag\")\n",
    "\n",
    "#Remove all non-argumentative sentences from the train and test pool. This is to simulate the Identification process identifying the argumentative sentences.\n",
    "\n",
    "non_argumentative_train = train[ train['Argumentative Label'] == '0'].index\n",
    "train.drop(non_argumentative_train, inplace = True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "non_argumentative_test = test[ test['Argumentative Label'] == '0'].index\n",
    "test.drop(non_argumentative_test, inplace = True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tokenisation_features(train)\n",
    "part_of_speech_features(train)\n",
    "positional_features(train)\n",
    "first_person_indicators_features(train)\n",
    "forward_indicator_feature(train)\n",
    "backward_indicator_feature(train)\n",
    "thesis_indicator_feature(train)\n",
    "\n",
    "tokenisation_features(test)\n",
    "part_of_speech_features(test)\n",
    "positional_features(test)\n",
    "first_person_indicators_features(test)\n",
    "forward_indicator_feature(test)\n",
    "backward_indicator_feature(test)\n",
    "thesis_indicator_feature(test)\n",
    "\n",
    "tokenisation_features(test_essay)\n",
    "part_of_speech_features(test_essay)\n",
    "positional_features(test_essay)\n",
    "first_person_indicators_features(test_essay)\n",
    "forward_indicator_feature(test_essay)\n",
    "backward_indicator_feature(test_essay)\n",
    "thesis_indicator_feature(test_essay)\n",
    "print(test_essay)\n",
    "\n",
    "\n",
    "\n",
    "#Y should be the argument component type label encoded - labels being MajorClaim, Claim and Premise\n",
    "component_type = preprocessing.LabelEncoder()\n",
    "#for some reason, MajorClaim = 1 while Claim = 0. Unsure why this is but keep in mind for testing label encoding\n",
    "component_type.fit(['MajorClaim','Claim', 'Premise'])\n",
    "\n",
    "x = train.loc[:, feature_columns]\n",
    "print(x)\n",
    "y = train.loc[:, ['Argument Component Type']]\n",
    "y_encoded = component_type.transform(y)\n",
    "y['Argument Component Type'] = y_encoded\n",
    "\n",
    "x_new = test.loc[:, feature_columns]\n",
    "print(x_new)\n",
    "y_new = test.loc[:, ['Argument Component Type']]\n",
    "print(y_new)\n",
    "y_new_encoded = component_type.transform(y_new)\n",
    "print(y_new_encoded)\n",
    "y_new['Argument Component Type'] = y_new_encoded\n",
    "\n",
    "tf = TfidfVectorizer(max_features = 800,strip_accents = 'ascii',stop_words = 'english',)\n",
    "\n",
    "x_sentences = x['Lemmatized Sentence']\n",
    "\n",
    "x_sentences_vectorized = tf.fit_transform(x_sentences)\n",
    "x_vectorized_dataframe = pd.DataFrame(x_sentences_vectorized.todense(), columns=tf.get_feature_names())\n",
    "x_concat = pd.concat([x, x_vectorized_dataframe], axis=1)\n",
    "x_final = x_concat.drop(['Lemmatized Sentence'], axis=1)\n",
    "\n",
    "x_new_sentences = x_new['Lemmatized Sentence']\n",
    "\n",
    "x_new_sentences_vectorized = tf.transform(x_new_sentences)\n",
    "x_new_vectorized_dataframe = pd.DataFrame(x_new_sentences_vectorized.todense(), columns=tf.get_feature_names())\n",
    "x_new_concat = pd.concat([x_new, x_new_vectorized_dataframe], axis=1)\n",
    "x_new_final = x_new_concat.drop(['Lemmatized Sentence'], axis=1)\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(x_final,y.values.ravel())\n",
    "\n",
    "pickle.dump(tf, open(\"tfidf_lemmatized.pickle\", \"wb\"))\n",
    "pickle.dump(component_type, open(\"component_type_encoder.pickle\", \"wb\"))\n",
    "pickle.dump(naive_bayes, open(\"component_classification_model.pickle\", \"wb\"))\n",
    "\n",
    "predictions = naive_bayes.predict(x_new_final)\n",
    "\n",
    "baseline = predictions\n",
    "baseline = np.where(baseline < 2, 2, baseline)\n",
    "\n",
    "c_m = confusion_matrix(y_new.values.ravel(), predictions)\n",
    "c_m_true = confusion_matrix(y_new.values.ravel(), y_new.values.ravel())\n",
    "print('Predicted Values: ', predictions)\n",
    "print('Accuracy score: ', accuracy_score(y_new.values.ravel(), predictions))\n",
    "print('Precision score: ', precision_score(y_new.values.ravel(), predictions, average='weighted'))\n",
    "print('Recall score: ', recall_score(y_new.values.ravel(), predictions, average='weighted'))\n",
    "print('Baseline Accuracy score: ', accuracy_score(y_new.values.ravel(), baseline))\n",
    "print('Baseline Precision score: ', precision_score(y_new.values.ravel(), baseline, average='weighted'))\n",
    "print('Baseline Recall score: ', recall_score(y_new.values.ravel(), baseline, average='weighted'))\n",
    "print('Confusion Matrix:')\n",
    "print(c_m)\n",
    "print('Actual Result Matrix:')\n",
    "print(c_m_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
